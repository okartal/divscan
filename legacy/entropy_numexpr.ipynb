{
 "metadata": {
  "name": "",
  "signature": "sha256:ef1dc4b988a10d79054cb317a0fb135ade1040b88bd2a0f4b5f3e140a68f7d52"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sympy import integrate, symbols\n",
      "from math import factorial"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, K, N = symbols('m K N')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def postprob(K, N, interval):\n",
      "    const = factorial(N+1)/(factorial(K)*factorial(N-K))\n",
      "    expr = const*m**K*(1-m)**(N-K)\n",
      "    return integrate(expr, (m, interval[0], interval[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numexpr as ne\n",
      "from scipy.special import xlogy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make a random array of pmf's (one pmf per line) and test the new entropy function\n",
      "d = np.random.rand(1000,2)\n",
      "p = (d.T/d.sum(axis=1)).T\n",
      "w = np.random.rand(1000)\n",
      "w = w/w.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def entropy(prob):\n",
      "    return ne.evaluate(\"sum(where(prob > 0, -prob*log(prob), 0), axis=1)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entropy(p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "array([ 0.47356996,  0.45966014,  0.24440429,  0.43816149,  0.20960672,\n",
        "        0.4948754 ,  0.25822589,  0.69192031,  0.69294931,  0.68084416])"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kldiv(p, q):\n",
      "    return ne.evaluate('sum(where(p > 0, p*log(p), 0) - where(q > 0, p*log(q), 0), axis=1)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jsdiv(p, w):\n",
      "    mixture = w.dot(p)\n",
      "    avg_kldiv = w.dot(kldiv(p, mixture))\n",
      "    return avg_kldiv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jsdiv2(p, w):\n",
      "    mixture = w.dot(p)\n",
      "    mix_entropy = entropy([mixture])\n",
      "    avg_entropy = w.dot(entropy(p))\n",
      "    return (mix_entropy - avg_entropy)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jsd(distributions=None, weights=None):\n",
      "    \"\"\" Calculates the Jensen-Shannon Divergence.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    d : array_like\n",
      "        d.shape = (k,n)\n",
      "        n distributions over k-dimensional sample space\n",
      "    w : array_like\n",
      "        w.shape = (n,)\n",
      "        weights for n distributions\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    j : float\n",
      "        Jensen-Shannon Divergence between the distributions\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The Jensen-Shannon Divergence is given by J = H(<P>) - <H>, the\n",
      "    entropy of the weighted average distribution minus the weighted\n",
      "    average of the distribution entropies. It is a measure of the\n",
      "    overall difference between distributions based on information\n",
      "    loss.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    d = distributions\n",
      "    w = weights\n",
      "\n",
      "    dAvg = d.dot(w)\n",
      "    h_dAvg = -xlogy(dAvg, dAvg).sum()\n",
      "    hAvg = -xlogy(w*d, d).sum()\n",
      "    j = h_dAvg - hAvg\n",
      "    return j"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jsdiv3(p, w):\n",
      "    return w.dot(np.sum(xlogy(p, p) - xlogy(p, w.dot(p)), axis=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print jsd(distributions=p.T, weights=w), \"\\n\", jsdiv(p, w), \"\\n\", jsdiv2(p,w), \"\\n\", jsdiv3(p,w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0473321885448 \n",
        "0.0473321885448 \n",
        "0.0473321885448 \n",
        "0.0473321885448\n"
       ]
      }
     ],
     "prompt_number": 119
    }
   ],
   "metadata": {}
  }
 ]
}