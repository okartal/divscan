#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""shannon

This file collects functions and the command-line parser for the shannon
program. shannon uses count data over genomic coordinates to calculate
information-theoretic measures of diversity.

Note: Eventually, functions that turn out to be reusable will become part of a
module.
"""

__author__ = 'Ã–nder Kartal'
__version__ = "0.1.1"

import argparse
import collections
import functools
import glob
import math
import os
import subprocess
import sys

import numpy as np
import pandas as pd
import numexpr as ne

def shannon(metadata=None, chrom=None, query=None, groupby=None, outfile=None):
    """Process the command-line arguments.
    """

    try:
        population = pd.read_csv(metadata, comment='#', sep='\t', header=0)
    except FileNotFoundError as e:
        print(e)
        sys.exit(1)

    if query:
        population.query(query, inplace=True)

    if groupby:
        metapopulation = population.groupby(groupby)
        for key, subpop in metapopulation:
            filename = groupname(by=groupby, name=key, fname=outfile)
            write_div(population=subpop, chrom=chrom, filename=filename)
    else:
        filename = outfile
        write_div(population=population, chrom=chrom, filename=filename)
    pass

def write_div(population=None, chrom=None, filename=None):
    """Write divergence for population and chrom into filename.
    """

    div = divergence(population, chrom, filename, method='jsd')
    withheader = not os.path.isfile(filename)
    div.to_csv(filename, header=withheader, sep='\t', index=True, mode='a')

    pass

def divergence(pop=None, chrom=None, filename=None, imputation=False,
               method='jsd', filetype='bismark_coverage'):
    """Computes within-group divergence for population.
    """

    input_files = pop['url']
    labels = pop['label']

    column = collections.OrderedDict()

    if filetype == 'bismark_coverage':
        column[0] = '#chrom'
        column[1] = 'start'
        column[2] = 'end'
        column[4] = 'E' # my single letter code for methylated C
        column[5] = 'C'
        types = [str, np.int64, np.int64, np.int32, np.int32]
        dtype = dict(zip(column.values(), types))
        alphabet = ['E', 'C']
        coordinate = [column[i] for i in range(3)]

    sup_chrom = chrom_supremum(input_files, chrom)
    sup_nsites = nsites_supremum(input_files, chrom)

    LOAD = 1e7
    chunkload = math.ceil(sup_chrom/sup_nsites * LOAD)
    chunksize = chunkload if chunkload < sup_chrom else sup_chrom

    windows = zip(range(1, sup_chrom, chunksize),
               range(1 + chunksize, sup_chrom + chunksize, chunksize))

    for interval in windows:

        ### input ###

        region = chrom + ':{0}-{1}'.format(*interval)

        tabix_query = (subprocess.Popen(['tabix', f, region],
                                        stdout=subprocess.PIPE,
                                        universal_newlines=True)
                       for f in input_files)

        dataframes = (pd.read_table(query.stdout, comment='#', header=None,
                                    usecols=list(column.keys()),
                                    names=list(column.values()), dtype=dtype,
                                    index_col=[0, 1, 2])
                      for query in tabix_query)

        indata = pd.concat(dataframes, axis=1, keys=labels,
                           names=['sample', 'alphabet'])

        ### process ###

        if indata.empty:
            continue

        if imputation:
            impute_value = impute(indata, method='pseudocount')
            indata.fillna(impute_value, inplace=True)

        return group_statistics(indata)

def groupname(by=None, name=None, fname=None):
    """Return filename of the subgroup.
    """

    # ensure that name is a tuple of strings
    name = tuple(str(key) for key in name)

    group = '_and_'.join('_'.join(items) for items in zip(by, name))

    old_suffix = fname.split('.')[-1]

    new_suffix = '.'.join([group, old_suffix])

    return fname.replace(old_suffix, new_suffix)

def chrom_supremum(tabixfiles, chrom):
    """Return the least upper bound for the chrom end coordinate.
    """

    end_coordinate = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, chrom], stdout=subprocess.PIPE)
        tail = subprocess.Popen(["tail", "-1"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        cut = subprocess.Popen(["cut", "-f3"], stdin=tail.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow first process to receive a SIGPIPE if process 2 exits.
        base_position = int(cut.communicate()[0])
        end_coordinate.append(base_position)

    return np.max(end_coordinate)

def nsites_supremum(tabixfiles, chrom):
    '''Return the least upper bound for the number of covered sites.
    '''

    sites = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, chrom], stdout=subprocess.PIPE)
        wcl = subprocess.Popen(["wc", "-l"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow tabix to receive a SIGPIPE if wcl exits.
        site_count = int(wcl.communicate()[0])
        sites.append(site_count)

    return np.max(sites)

def entropy(counts, axis=1, method='mle'):
    """Entropy of probability mass functions.

    TODO
    - specify units in terms of log base
    - base e: nat
    - base 2: bit or shannon (Sh)
    """

    expression = "sum(where(pr > 0, -pr * log(pr), 0), axis={})".format(axis)

    if method == 'mle':
        '''Maximum-likelihood/plug-in/non-parametric estimator
        '''
        sumCounts = counts.sum(axis)[..., np.newaxis]
        pr = counts/sumCounts
        result = ne.evaluate(expression)

    return result

def impute(data, method='pseudocount'):
    if method == 'pseudocount':
        # given by the parameters of a uninformative Dirichlet prior on the probabilities
        value = 1
    return value

def group_statistics(indata):
    """Compute output for within-group JS divergence.

    This functions calculates the within-group Jensen-Shannon divergence (JSD)
    over a region using the counts over an alphabet for each member of the
    population sample and each genomic position. It also outputs some quantities
    to allow later calculations of between-group JSD (HMIX, E, C). Furthermore,
    population sample size is calculated to exclude positions with sample size =
    1 and to record it for assessing imbalance and uncertainty when comparing
    two groups for between-group JSD.

    Arguments
    ---------

    data : pandas DataFrame

    Returns
    -------

    pandas DataFrame for within-group JSD

    - sample size : number of samples with coverage > 0
    - E : number of methylated C base counts over all samples
    - C : number of unmethylated C base counts over all samples
    - average methylation : weighted average methylation level over all samples
    - mixture entropy : entropy of the mixture distribution
    - JS divergence : mixture entropy - weighted average entropy of all distributions

    """

    # coverage over population sample: this quantity is needed for several
    # downstream calculations
    coverage_sample_all = indata.sum(axis=1, level='sample')
    sample_size = coverage_sample_all.notnull().sum(axis=1)

    data = indata[sample_size > 1]
    coverage_sample = coverage_sample_all[sample_size > 1]

    # output: coverage over alphabet and mixture entropy
    coverage_alphabet = data.sum(axis=1, level='alphabet').astype(np.int64)

    mix_entropy = entropy(coverage_alphabet.values)

    # average entropy over population sample: is needed alongside mix_entropy to
    # calculate JS divergence
    counts = data.values.reshape(coverage_sample.shape[0],
                                 coverage_sample.shape[1],
                                 coverage_alphabet.shape[1])

    avg_entropy = np.average(entropy(counts, axis=2),
                             weights=coverage_sample.fillna(0),
                             axis=1)

    # initialize output df
    pop = pd.DataFrame({'JSD': mix_entropy - avg_entropy, 'HMIX': mix_entropy},
                       index=data.index).round(decimals=3)

    pop = pd.concat([pop, coverage_alphabet], axis=1)

    pop['sample size'] = sample_size[sample_size > 1]

    pop['members'] = (
        coverage_sample.apply(lambda x: ','.join(x.dropna().index),axis=1))

    return pop

if __name__ == '__main__':

    parser = argparse.ArgumentParser()

    parser.prog = 'shannon'

    parser.description = ('%(prog)s: a program to measure population diversity'
                          ' using information-theory.')

    parser.add_argument('-v', '--version', action='version',
                        version='%(prog)s {0}'.format(__version__))

    parser.add_argument('-c', '--chrom', required=True, type=str,
                        help='''
                        Chromosome name.
                        ''')

    parser.add_argument('-i', '--input', metavar="URL", required=True,
                        help='''
                        A tab-separated table with metadata for each
                        sample/record. Lines starting with # are ignored. The
                        first line is interpreted as the header. The columns
                        "url" and "label" are mandatory and the corresponding
                        fields have to be non-empty for each sample.
                        ''')

    parser.add_argument('-q', '--query', metavar='"STR"', type=str,
                        help='''
                        Query expression to select a subset of samples. The
                        expression has to be in double quotes. Examples: "tissue ==
                        'leaf'", "age >= 32".
                        ''')

    parser.add_argument('-g', '--groupby', metavar='STR', nargs='+', type=str,
                        help='''
                        The factor according to which the selected set is
                        partitioned; has to match column names in the input
                        metadata. One or more factors can be given, which
                        produces a file for each combination of factors.
                        ''')

    parser.add_argument('-o', '--output', metavar='FILE',
                        help='Name of the output file.')

    args = parser.parse_args()

    shannon(metadata=args.input, chrom=args.chrom, query=args.query,
            groupby=args.groupby, outfile=args.output)
