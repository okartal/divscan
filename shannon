#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""shannon

This file collects functions and the command-line parser for the shannon
program. shannon uses count data over genomic coordinates to calculate
information-theoretic measures of diversity.

Note: Eventually, functions that turn out to be reusable will become part of a
module.
"""

__author__ = 'Ã–nder Kartal'
__version__ = "0.1.1"

import argparse
import collections
import functools
import glob
import math
import os
import subprocess
import sys

import numpy as np
import pandas as pd
import numexpr as ne

def shannon(metadata=None, chrom=None, query=None, groupby=None, outfile=None):
    """Process the command-line arguments.
    """

    try:
        population = pd.read_csv(metadata, comment='#', sep='\t', header=0)
    except FileNotFoundError as e:
        print(e)
        sys.exit(1)

    if query:
        population.query(query, inplace=True)

    if groupby:
        metapopulation = population.groupby(groupby)
        for key, subpop in metapopulation:
            filename = groupname(by=groupby, name=key, fname=outfile)
            write_div(population=subpop, chrom=chrom, filename=filename)
    else:
        filename = outfile
        write_div(population=population, chrom=chrom, filename=filename)
    pass

def write_div(population=None, chrom=None, filename=None):
    """Write divergence for population and chrom into filename.
    """

    div = divergence(population, chrom, filename, method='jsd')
    withheader = not os.path.isfile(filename)
    div.to_csv(filename, header=withheader, sep='\t', index=True, mode='a')

    pass

def divergence(pop=None, chrom=None, filename=None, imputation=False,
               method='jsd', filetype='bismark_coverage'):
    """Computes within-group divergence for population.
    """

    input_files = pop['url']
    labels = pop['label']

    column = collections.OrderedDict()

    if filetype == 'bismark_coverage':
        column[0] = '#chrom'
        column[1] = 'start'
        column[2] = 'end'
        column[4] = 'E' # my single letter code for methylated C
        column[5] = 'C'
        types = [str, np.int64, np.int64, np.int32, np.int32]
        dtype = dict(zip(column.values(), types))
        alphabet = ['E', 'C']
        coordinate = [column[i] for i in range(3)]

    SIZE = 1e4

    chrom_sup = chrom_supremum(input_files, chrom)
    nsites_sup = nsites_supremum(input_files, chrom)
    chunksize = math.ceil(chrom_sup/nsites_sup * SIZE)

    grid = zip(range(1, chrom_sup, chunksize),
               range(1 + chunksize, chrom_sup + chunksize, chunksize))

    for interval in grid:

        ### input ###

        region = chrom + ':{0}-{1}'.format(*interval)

        tabix_query = (subprocess.Popen(['tabix', f, region],
                                        stdout=subprocess.PIPE,
                                        universal_newlines=True)
                       for f in input_files)

        dataframes = (pd.read_table(query.stdout, skiprows=1, header=None,
                                    usecols=list(column.keys()),
                                    names=list(column.values()), dtype=dtype,
                                    index_col=[0, 1, 2])
                      for query in tabix_query)

        indata = pd.concat(dataframes, axis=1, keys=labels)

        ### process ###

        if indata.empty:
            continue

        if imputation:
            impute_value = impute(indata, method='pseudocount')
            indata.fillna(impute_value, inplace=True)

        jsd_stats = group_statistics(indata)

        return pd.DataFrame(jsd_stats, index=indata.index)

def groupname(by=None, name=None, fname=None):
    """Return filename of the subgroup.
    """

    # ensure that name is a tuple of strings
    name = tuple(str(key) for key in name)

    group = '_and_'.join('_'.join(items) for items in zip(by, name))

    old_suffix = fname.split('.')[-1]

    new_suffix = '.'.join([group, old_suffix])

    return fname.replace(old_suffix, new_suffix)

def chrom_supremum(tabixfiles, chrom):
    """Return the least upper bound for the chrom end coordinate.
    """

    end_coordinate = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, chrom], stdout=subprocess.PIPE)
        tail = subprocess.Popen(["tail", "-1"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        cut = subprocess.Popen(["cut", "-f3"], stdin=tail.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow first process to receive a SIGPIPE if process 2 exits.
        base_position = int(cut.communicate()[0])
        end_coordinate.append(base_position)

    return np.max(end_coordinate)

def nsites_supremum(tabixfiles, chrom):
    '''Return the least upper bound for the number of covered sites.
    '''

    sites = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, chrom], stdout=subprocess.PIPE)
        wcl = subprocess.Popen(["wc", "-l"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow tabix to receive a SIGPIPE if wcl exits.
        site_count = int(wcl.communicate()[0])
        sites.append(site_count)

    return np.max(sites)

def entropy(counts, axis=1, method='mle'):
    """Entropy of probability mass functions.

    TODO
    - specify units in terms of log base
    - base e: nat
    - base 2: bit or shannon (Sh)
    """

    expression = "sum(where(pr > 0, -pr * log(pr), 0), axis={})".format(axis)

    if method == 'mle':
        '''Maximum-likelihood/plug-in/non-parametric estimator
        '''
        sumCounts = counts.sum(axis)[..., np.newaxis]
        pr = counts/sumCounts
        result = ne.evaluate(expression)

    return result

def impute(data, method='pseudocount'):
    if method == 'pseudocount':
        # given by the parameters of a uninformative Dirichlet prior on the probabilities
        value = 1
    return value

def group_statistics(data):
    """Returns statistics for JS divergence.

    This functions uses numpy to return the within-group
    JS-divergence. Some other statistics are calculated to assess
    uncertainty as well as between-group divergence for a future
    function if a second file is given.

    Arguments
    ---------

    data : pandas DataFrame

    Returns
    -------

    stats : dictionary with statistics for each position

    - sample size : number of samples with coverage > 0
    - E : number of methylated C base counts over all samples
    - C : number of unmethylated C base counts over all samples
    - average methylation : weighted average methylation level over all samples
    - mixture entropy : entropy of the mixture distribution
    - JS divergence : mixture entropy - weighted average entropy of all distributions

    """

    stats = dict()

    # input

    samples = data.columns.levels[0]
    alphabet = data.columns.levels[1]
    counts = data.values.reshape(-1, len(samples), len(alphabet))

    # some necessary calculations

    sum_alphabet = np.nansum(counts, axis=1)
    sum_sample = np.nansum(counts, axis=2)
    entropy_sample = entropy(counts, axis=2)
    weight_sample = sum_sample/sum_sample.sum(axis=1)[..., np.newaxis]

    # assignments

    stats['sample size'] = np.sum(np.all(~np.isnan(counts), axis=2), axis=1)

    for ix, base in enumerate(alphabet):
        # assuming that order of bases is the same in alphabet and subarray
        stats[base] = np.uint(sum_alphabet[:, ix])

    total_count = stats['E'] + stats['C']
    average_entropy = np.average(entropy_sample, weights=weight_sample, axis=1)

    stats['average methylation'] = stats['E']/total_count

    stats['mixture entropy'] = entropy(sum_alphabet)

    stats['JS divergence'] = stats['mixture entropy'] - average_entropy

    return stats

if __name__ == '__main__':

    parser = argparse.ArgumentParser()

    parser.prog = 'shannon'

    parser.description = ('%(prog)s: a program to measure population diversity'
                          ' using information-theory.')

    parser.add_argument('-v', '--version', action='version',
                        version='%(prog)s {0}'.format(__version__))

    parser.add_argument('-c', '--chrom', required=True, type=str,
                        help='''
                        Chromosome name.
                        ''')

    parser.add_argument('-i', '--input', metavar="URL", required=True,
                        help='''
                        A tab-separated table with metadata for each
                        sample/record. Lines starting with # are ignored. The
                        first line is interpreted as the header. The columns
                        "url" and "label" are mandatory and the corresponding
                        fields have to be non-empty for each sample.
                        ''')

    parser.add_argument('-q', '--query', metavar='"STR"', type=str,
                        help='''
                        Query expression to select a subset of samples. The
                        expression has to be in double quotes. Examples: "tissue ==
                        'leaf'", "age >= 32".
                        ''')

    parser.add_argument('-g', '--groupby', metavar='STR', nargs='+', type=str,
                        help='''
                        The factor according to which the selected set is
                        partitioned; has to match column names in the input
                        metadata. One or more factors can be given, which
                        produces a file for each combination of factors.
                        ''')

    parser.add_argument('-o', '--output', metavar='FILE',
                        help='Name of the output file.')

    args = parser.parse_args()

    shannon(metadata=args.input, chrom=args.chrom, query=args.query,
            groupby=args.groupby, outfile=args.output)
