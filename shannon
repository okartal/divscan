#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""shannon

This file collects functions and the command-line parser for the shannon
program. shannon uses count data over genomic coordinates to calculate
information-theoretic measures of diversity.

Note: Eventually, functions that turn out to be reusable will become part of a
module.
"""

__author__ = 'Ã–nder Kartal'

import argparse
import collections
import functools
import glob
import math
import os
import subprocess
import sys
from urllib.parse import urlparse

try:
    import numpy as np
except ImportError:
    print("numpy is missing!")

try:
    import pandas as pd
except ImportError:
    print("pandas is missing!")

try:
    import numexpr as ne
except ImportError:
    print("numexpr is missing!")

def shannon():
    '''Command line interface for shannon script.
    '''

    parser = argparse.ArgumentParser()

    parser.prog = 'shannon'
    parser.description = ('shannon: information-theory to quantify genomic'
                          ' population diversity.')


    parser.add_argument('-c', '--contig', required=True, type=str,
                        help='''
                        Contig ID for which computations are done.
                        ''')
    parser.add_argument('-i', '--input', required=True, nargs='+',
                        help='''
                        A list of filepaths or a pattern with wildcard that
                        returns an appropriate list of files.
                        ''')
    parser.add_argument('-v', '--version', action='version',
                        version='%(prog)s 0.1.0')
    parser.add_argument('-o', '--output', metavar='FILE')
    parser.add_argument('-m', '--metadata', metavar='FILE',
                        help='''
                        A tab-separated text file that contains population
                        metadata. Entries in a column named 'label' have to
                        match the prefix of the input files.
                        ''')
    parser.add_argument('-q', '--query', metavar='"STR"', type=str,
                        help='''
                        Expression in double quotes that defines the set on
                        which the diversity index is calculated. Is ignored when
                        metadata is missing.
                        ''')
    parser.add_argument('-g', '--groupby', metavar='STR', nargs='+', type=str,
                        help='''
                        The variable or factor/relation according to which the
                        query set is partitioned. Is ignored when
                        metadata is missing.
                        ''')

    process_arguments(args=parser.parse_args())
    
    pass

def process_arguments(args=None):
    '''Process command-line input.
    '''

    print('processing...')
    
    suffix = "bed"

    if not args.output:
        # give it a default generic name
        args.output = ".".join([args.contig, suffix])

    # TODO: use path module to properly handle path objects instead of string-fu
    if not args.metadata:
        if (args.query or args.groupby):
            print('[INPUT ERROR]')
            print('Please supply --metadata if you want to use --query and/or --groupby!')
            sys.exit()
        else:
            filenames = get_filenames(args.input)
            labels = [f.split('.')[0] for f in filenames]
            process_data(filenames, labels=labels, args=args)
    elif args.metadata:
        meta = pd.read_table(args.metadata, header=0)
        files = get_filenames(args.input) # make input mutually exclusive with args.metadata
        if args.query and not args.groupby:
            queryset = meta.query(args.query)
            labels = queryset["label"]
            filenames = [name for name in files if any(name.find(label) != -1 for label in labels)]
            args.output = ".".join([args.contig, args.query, suffix]).replace("==", "_").replace("'", "")
            process_data(filenames, labels=queryset["label"], args=args)
            print('[QUERY DATA]')
            print("\n".join(filenames))
            print('[OUTPUT FILE]')
            print(args.output)
        if args.groupby:
            if not args.query:
                quotientset = meta.groupby(args.groupby)
            else:
                quotientset = meta.query(args.query).groupby(args.groupby)

            for key, group in quotientset:
                labels = group["label"]
                filenames = [name for name in files if any(name.find(label) != -1 for label in labels) and name.endswith('.gz')]
                if type(key) is not tuple:
                    key = (str(key), )
                else:
                    key = tuple(str(el) for el in key)
                groups = list(zip(args.groupby, key))
                groupby = "^".join(["_".join(g) for g in groups])
                if args.query:
                    args.output = ".".join([args.contig, args.query, groupby, suffix])
                else:
                    args.output = ".".join([args.contig, groupby, suffix])
                args.output = args.output.replace("==", "_").replace("'", "")

                process_data(filenames, labels=labels, args=args)

                print('[GROUP DATA]')
                print("\n".join(filenames))
                print('[GROUP FILE]')
                print(args.output)

        if not args.query and not args.groupby:
            print('[NOTE]')
            print('Metadata is ignored if no --query or --groupby is given.\n'
                  'We assume that the file prefix is the label of the sample.')
            filenames = files
            labels = [f.split('.')[0] for f in filenames]
            process_data(filenames, labels, args)

    pass

def get_filenames(file_input, metadata=None):
    '''Checks the file input to get the list of filenames.
    '''

    filenames = []

    if len(file_input) > 1:
        filenames = file_input
    elif len(file_input) == 1:
        try:
            filenames = os.listdir(*file_input)
        except NotADirectoryError as e:
            print('[INPUT ERROR]')
            print('There is only one data file!')
            sys.exit()
        except FileNotFoundError as e:
            # we assume first that it is a pattern with wildcard
            filenames = glob.glob(*file_input)
            if not filenames:
                print('[INPUT ERROR]')
                print('There are no data files to read!')
                sys.exit()

    print('[INPUT DATA]')
    print(*filenames, sep='\n')

    return filenames

def contig_supremum(tabixfiles, contig):
    """Return the least upper bound for the contig end coordinate.
    """

    end_coordinate = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, contig], stdout=subprocess.PIPE)
        tail = subprocess.Popen(["tail", "-1"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        cut = subprocess.Popen(["cut", "-f3"], stdin=tail.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow first process to receive a SIGPIPE if process 2 exits.
        base_position = int(cut.communicate()[0])
        end_coordinate.append(base_position)

    return np.max(end_coordinate)

def nsites_supremum(tabixfiles, contig):
    '''Return the least upper bound for the number of covered sites.
    '''

    sites = list()

    for f in tabixfiles:
        tabix = subprocess.Popen(["tabix", f, contig], stdout=subprocess.PIPE)
        wcl = subprocess.Popen(["wc", "-l"], stdin=tabix.stdout, stdout=subprocess.PIPE)
        tabix.stdout.close()  # Allow tabix to receive a SIGPIPE if wcl exits.
        site_count = int(wcl.communicate()[0])
        sites.append(site_count)

    return np.max(sites)

def entropy(counts, axis=1, method='mle'):
    """Entropy of probability mass functions.

    TODO
    - specify units in terms of log base
    """

    expression = "sum(where(pr > 0, -pr * log(pr), 0), axis={})".format(axis)

    if method == 'mle':
        '''Maximum-likelihood/plug-in/non-parametric estimator
        '''
        sumCounts = counts.sum(axis)[..., np.newaxis]
        pr = counts/sumCounts
        result = ne.evaluate(expression)

    return result

def impute(data, method='pseudocount'):
    if method == 'pseudocount':
        # given by the parameters of a uninformative Dirichlet prior on the probabilities
        value = 1
    return value

def group_statistics(data):
    """Returns statistics for JS divergence.

    This functions uses numpy to return the within-group
    JS-divergence. Some other statistics are calculated to assess
    uncertainty as well as between-group divergence for a future
    function if a second file is given.

    Arguments
    ---------

    data : pandas DataFrame

    Returns 
    -------

    stats : dictionary with statistics for each position

    - sample size : number of samples with coverage > 0
    - E : number of methylated C base counts over all samples
    - C : number of unmethylated C base counts over all samples
    - average methylation : weighted average methylation level over all samples
    - mixture entropy : entropy of the mixture distribution
    - JS divergence : mixture entropy - weighted average entropy of all distributions

    """

    DECIMALS = 5
    
    stats = dict()
    
    # input

    samples = data.columns.levels[0]
    alphabet = data.columns.levels[1]
    counts = data.values.reshape(-1, len(samples), len(alphabet))

    # some necessary calculations

    sum_alphabet = np.nansum(counts, axis=1)
    sum_sample = np.nansum(counts, axis=2)
    entropy_sample = entropy(counts, axis=2)
    weight_sample = sum_sample/sum_sample.sum(axis=1)[..., np.newaxis]

    # assignments

    stats['sample size'] = np.sum(np.all(~np.isnan(counts), axis=2), axis=1)

    for ix, base in enumerate(alphabet):
        # assuming that order of bases is the same in alphabet and subarray
        stats[base] = np.uint(sum_alphabet[:, ix])

    total_count = stats['E'] + stats['C']
    average_entropy = np.average(entropy_sample, weights=weight_sample, axis=1)

    stats['average methylation'] = np.round(stats['E']/total_count, DECIMALS)

    stats['mixture entropy'] = np.round(entropy(sum_alphabet), DECIMALS)

    stats['JS divergence'] = np.round(stats['mixture entropy'] - average_entropy, 4)

    return stats

def process_data(filenames, labels=None, args=None, filetype='bismark_coverage'):
    '''Combine input data to read count information.

    Parameters
    ----------

    Returns
    -------

    Notes
    -----

    '''

    imputation = False

    outfile = args.output

    column = collections.OrderedDict()

    if filetype == 'bismark_coverage':
        column[0] = '#chrom'
        column[1] = 'start'
        column[2] = 'end'
        column[4] = 'E' # my single letter code for methylated C
        column[5] = 'C'
        types = [str, np.int64, np.int64, np.int32, np.int32]
        dtype = dict(zip(column.values(), types))
        alphabet = ['E', 'C']
        coordinate = [column[i] for i in range(3)]

    SIZE = 1e5
    
    contig_sup = contig_supremum(filenames, args.contig)
    nsites_sup = nsites_supremum(filenames, args.contig)
    chunksize = math.ceil(contig_sup/nsites_sup * SIZE)

    grid = zip(range(1, contig_sup, chunksize),
               range(1 + chunksize, contig_sup + chunksize, chunksize))

    #with open(args.output, 'a') as outfile:
        #trackline = 'track type=bedGraph\n'
        #outfile.write(trackline)

    for interval in grid:

        ### input ###

        region = args.contig + ':{0}-{1}'.format(*interval)

        tabix_query = (subprocess.Popen(['tabix', f, region],
                                        stdout=subprocess.PIPE,
                                        universal_newlines=True)
                       for f in filenames)

        dataframes = (pd.read_table(query.stdout, skiprows=1, header=None,
                                    usecols=list(column.keys()),
                                    names=list(column.values()), dtype=dtype,
                                    index_col=[0, 1, 2])
                      for query in tabix_query)

        indata = pd.concat(dataframes, axis=1, keys=labels)

        ### process ###
        
        if indata.empty:
            continue

        if imputation:
            impute_value = impute(indata, method='pseudocount')
            indata.fillna(impute_value, inplace=True)

        jsd_stats = group_statistics(indata)

        ### output ###

        outdata = pd.DataFrame(jsd_stats, index=indata.index)

        if not os.path.isfile(outfile):
            # write to csv with header
            outdata.to_csv(outfile, header=True, sep='\t', index=True, mode='a')
        else:
            outdata.to_csv(outfile, header=False, sep='\t', index=True, mode='a')

    pass

if __name__ == '__main__':
    shannon()
